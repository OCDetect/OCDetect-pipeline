Index: data_preparation/prepare.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport re\nimport time\nimport yaml\nimport pandas as pd\nfrom typing import List\nfrom tqdm import tqdm\nfrom misc import logger\nfrom tsfresh.feature_extraction import extract_features, MinimalFCParameters\nfrom datetime import date\nfrom data_preparation.utils.filter import butter_filter\nfrom data_preparation.utils.scaler import std_scaling_data\n\nsave_data = True\noverwrite_data = True\n\n\ndef window_data(subject_recordings: List[pd.DataFrame], subject_id, settings: dict):\n    \"\"\"\n    Prepares the list with recordings for one subject by creating windows.\n    :param subject_recordings: A list with single recordings for a subject\n    :param subject_id: the ID for the subject\n    :param settings: the overall settings\n    :return: the data fragmented into windows\n    \"\"\"\n\n    logger.info(\"Window Data\")\n\n    # get predefined settings\n    window_size = settings.get(\"window_size\") * settings.get(\"sampling_frequency\")\n    overlap = settings.get(\"overlap\")\n    labelling_algorithm = settings.get(\"labelling_algorithm\")\n\n    stride = int(window_size * (1 - overlap))\n\n    window_labels = []\n    window_list = []\n    user_list = []\n\n    unique_id = 0  # for calculating tsfresh features\n\n    for recording in subject_recordings:\n        user_data_size = len(recording)\n\n        for w in tqdm(range(0, user_data_size - window_size + 1, stride)):\n            # copy to avoid warning that original dataset is changed\n            curr_window = recording.iloc[w:w + window_size].copy()\n            curr_window = curr_window.fillna(0)\n\n            # if current window only has entries that are labelled as to be ignored,\n            # do not consider this window any further\n            if check_ignore(curr_window):\n                continue\n\n            # Choose label\n            if labelling_algorithm == 'Majority':\n                majority_label = perform_majority_voting(curr_window)\n                window_labels.append(majority_label)\n\n                user_list.append(subject_id)\n            curr_window['tsfresh_id'] = unique_id\n\n            unique_id += 1\n            curr_window = curr_window[['acc x', 'acc y', 'acc z', 'gyro x', 'gyro y', 'gyro z', 'tsfresh_id']]\n            # pythons list append is much faster than pandas concat (time increased exponentially with concat)\n            window_list.append(curr_window)\n\n    return pd.concat(window_list), pd.Series(window_labels, index=None), pd.Series(user_list, index=None)\n\n\ndef check_ignore(current_window):\n    counts = current_window['ignore'].value_counts()\n    if counts.get(0, 0) > 0 or counts.get(1, 0) > 0:  # Todo: @ Robin, is that check sufficient?\n        return False\n    else:\n        return True\n\n\ndef perform_majority_voting(current_window, hw_general=True):\n    counts = current_window['relabeled'].value_counts()\n\n    if hw_general:\n        null_class = counts.get(0, 0)\n        routine_hw = counts.get(1, 0)\n        compulsive_hw = counts.get(2, 0)\n\n        if routine_hw + compulsive_hw > null_class:\n            majority_label = 1\n        else:\n            majority_label = 0\n    else:\n        null_class = counts.get(0, 0)\n        routine_hw = counts.get(1, 0)\n        compulsive_hw = counts.get(2, 0)\n\n        if routine_hw > compulsive_hw and routine_hw > null_class:\n            majority_label = 1  # routine hand washing present\n        elif compulsive_hw > routine_hw and compulsive_hw > null_class:\n            majority_label = 2  # compulsive hand washing present\n        else:\n            majority_label = 0  # null class\n\n    return majority_label\n\n\ndef feature_extraction(subject_windows: pd.DataFrame, settings):\n    \"\"\"\n    Extracts features using tsfresh\n    :param subject_windows: The windowed data for one subject\n    :param settings: the overall settings\n    :return: the extracted features in a list\n    \"\"\"\n\n    logger.info(\"Extracting Features\")\n    features_list = extract_features(subject_windows, column_id=settings.get(\"id\"),\n                                              default_fc_parameters=MinimalFCParameters(),\n                                              n_jobs=settings.get(\"jobs\"))\n\n    return features_list\n\n\ndef load_data_preparation_settings(settings: dict): # Todo: make sure that not over- AND undersampling are True\n    use_filter = settings.get(\"use_filter\")\n    use_scaling = settings.get(\"use_scaling\")\n    resample = settings.get(\"resample\")\n    use_undersampling = settings.get(\"use_undersampling\")\n    use_oversampling = settings.get(\"use_oversampling\")\n\n    return use_filter, use_scaling, resample, use_undersampling, use_oversampling\n\n\ndef get_data_path_variables(use_scaling, use_filter, config:dict, settings: dict):\n\n    export_path = config.get(\"export_subfolder_ml_prepared\")\n\n    window_size = settings.get(\"window_size\")\n    subjects = settings.get(\"use_ocd_only\")\n\n    subjects_folder_name = \"all_subjects\" if not subjects else \"ocd_diagnosed_only\"\n    sub_folder_path = f\"ws_{window_size}_s/{subjects_folder_name}\"\n\n    raw = \"_raw\" if settings.get(\"raw_features\") else \"\"\n\n    scaling = \"scaled\" if use_scaling and not raw else \"not_scaled\"\n    filtering = \"filtered\" if use_filter and not raw else \"not_filtered\"\n\n    return window_size, subjects, subjects_folder_name, sub_folder_path, export_path, scaling, filtering, raw\n\n\n# main function for data preparation\ndef prepare_data(settings: dict, config: dict, raw: bool=False):\n    use_filter, use_scaling, resample, use_undersampling, use_oversampling = load_data_preparation_settings(settings)\n    all_subjects = True if not settings.get(\"use_ocd_only\") else False\n\n    logger.info(\"Preparing data for machine learning\")\n\n    folder_path = config.get(\"export_subfolder\")\n    pattern = r'OCDetect_(\\d+)'\n\n    dataframes = {}\n\n    subject_numbers = settings.get(\"all_subjects\") if all_subjects else settings.get(\"ocd_diagnosed_subjects\")\n\n    for file_name in tqdm(os.listdir(folder_path)):\n        if file_name.endswith('.csv'):\n            match = re.search(pattern, file_name)\n            if match:\n                subject_number = int(match.group(1))\n\n                if subject_number in subject_numbers:\n                    file_path = os.path.join(folder_path, file_name)\n                    df = pd.read_csv(file_path, )\n\n                    if subject_number in dataframes:\n                        dataframes[subject_number].append(df)\n                    else:\n                        dataframes[subject_number] = [df]\n\n    subjects = dataframes.keys()\n    data = list(dataframes.values())\n\n    start_time = time.time()\n\n    # 1. Filter data if desired\n    if use_filter and not raw:\n        filtered_data_all = []\n        for subject in data:\n            filtered_data_subject = []\n            for recording in subject:\n                filtered_data_subject.append(butter_filter(recording, settings))\n            filtered_data_all.append(filtered_data_subject)\n        data = filtered_data_all\n\n    features = []\n    labels = []\n    users = []\n\n    logger.info(\"Windowing data\")\n    for i, subject_data in zip(subjects, data):\n        logger.info(f\"Subject: {i} ----\")\n\n        # 2. Do some preparations for good measure\n        logger.info(\"Sorting data\")\n        subject_data = sorted(subject_data, key=lambda x: pd.to_datetime(x.iloc[0].timestamp))\n\n        # 3. Window data\n        windows, user_labels, user_id = window_data(subject_data, i, settings)\n        labels.append(user_labels)\n        users.append(user_id)\n\n        logger.info(f\"Amount of data points : {len(windows)}\")\n        logger.info(f\"Amount of windows : {windows['tsfresh_id'].iloc[-1]}\")\n\n        # 4. Extracting features\n        features_user = feature_extraction(windows, settings) if not raw else windows\n        features.append(features_user)\n\n        logger.info(f\"Subject: {i}, features: {len(features_user)}, labels: {len(user_labels)}\")\n\n    labels = pd.concat(labels).reset_index(drop=True)\n    users = pd.concat(users).reset_index(drop=True)\n\n    features = pd.concat(features)\n    feature_names = features.columns.values.tolist()\n\n    # 5. Scale data if desired\n    if use_scaling and not raw:\n        features = std_scaling_data(features, settings)\n\n    end_time = time.time()\n    windowing_time_s = end_time - start_time\n    windowing_time_min = windowing_time_s / 60\n\n    if save_data:\n        window_size, subjects, subjects_folder_name, sub_folder_path, export_path, scaling, filtering, raw_str = get_data_path_variables(\n            use_scaling, use_filter, config, settings)\n\n        today = date.today()\n        file_date = today.strftime(\"%Y-%m-%d\")  # Format the date as \"YYYY-MM-DD\"\n\n        os.makedirs(f\"{export_path}/{sub_folder_path}\", exist_ok=True)\n\n        labels.to_csv(f\"{export_path}{sub_folder_path}/labels_{filtering}_{scaling}{raw_str}.csv\")\n        users.to_csv(f\"{export_path}{sub_folder_path}/users_{filtering}_{scaling}{raw_str}.csv\")\n        features.to_csv(f\"{export_path}{sub_folder_path}/features_{filtering}_{scaling}{raw_str}.csv\")\n        pd.DataFrame(feature_names).to_csv(f\"{export_path}{sub_folder_path}/feature_names_{filtering}_{scaling}{raw_str}.csv\")\n\n        # create file with meta information for the current window setup\n        meta_info = f\"Meta information for \\\"{file_date}\\\":\\n\"\n        meta_info += \"=\" * 40 + \"\\n\"\n        meta_info += f\"Time needed for window data: {windowing_time_s:.2f} seconds ({windowing_time_min:.2f} minutes) \\n\"\n        meta_info += \"-\" * 40 + \"\\n\"\n\n        settings_data = yaml.dump(settings)\n\n        meta_file = \"meta_info.txt\"\n        with open(f\"{export_path}/{sub_folder_path}/{meta_file}\", 'w') as file:\n            file.write(meta_info)\n            file.write(\"Used following settings file: \\n\")\n            file.write(settings_data)\n\n    return labels, features, users, feature_names
===================================================================
diff --git a/data_preparation/prepare.py b/data_preparation/prepare.py
--- a/data_preparation/prepare.py	
+++ b/data_preparation/prepare.py	
@@ -160,7 +160,7 @@
     dataframes = {}
 
     subject_numbers = settings.get("all_subjects") if all_subjects else settings.get("ocd_diagnosed_subjects")
-
+    print(folder_path)
     for file_name in tqdm(os.listdir(folder_path)):
         if file_name.endswith('.csv'):
             match = re.search(pattern, file_name)
